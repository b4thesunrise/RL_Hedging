{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSD Final Project\n",
    "\n",
    "## Title: Reinforcement Learning in Options Hedging \n",
    "\n",
    "### Team member: Jiang, T., Cui, D.F. and Wang, Y.Z.\n",
    "\n",
    "$$\n",
    "\\newcommand{\\supp}{\\mathrm{supp}}\n",
    "\\newcommand{\\E}{\\mathbb{E} }\n",
    "\\newcommand{\\Eof}[1]{\\mathbb{E}\\left[ #1 \\right]}\n",
    "\\def\\Cov{{ \\mbox{Cov} }}\n",
    "\\def\\Var{{ \\mbox{Var} }}\n",
    "\\newcommand{\\1}{\\mathbf{1} }\n",
    "\\newcommand{\\PP}{\\mathbb{P} }\n",
    "\\newcommand{\\Pof}[1]{\\mathbb{P}\\left[ #1 \\right]}\n",
    "%\\newcommand{\\Pr}{\\mathrm{Pr} }\n",
    "\\newcommand{\\QQ}{\\mathbb{Q} }\n",
    "\\newcommand{\\RR}{\\mathbb{R} }\n",
    "\\newcommand{\\DD}{\\mathbb{D} }\n",
    "\\newcommand{\\HH}{\\mathbb{H} }\n",
    "\\newcommand{\\spn}{\\mathrm{span} }\n",
    "\\newcommand{\\cov}{\\mathrm{cov} }\n",
    "\\newcommand{\\sgn}{\\mathrm{sgn} }\n",
    "\\newcommand{\\HS}{\\mathcal{L}_{\\mathrm{HS}} }\n",
    "%\\newcommand{\\HS}{\\mathrm{HS} }\n",
    "\\newcommand{\\trace}{\\mathrm{trace} }\n",
    "\\newcommand{\\LL}{\\mathcal{L} }\n",
    "%\\newcommand{\\LL}{\\mathrm{L} }\n",
    "\\newcommand{\\s}{\\mathcal{S} }\n",
    "\\newcommand{\\ee}{\\mathcal{E} }\n",
    "\\newcommand{\\ff}{\\mathcal{F} }\n",
    "\\newcommand{\\hh}{\\mathcal{H} }\n",
    "\\newcommand{\\bb}{\\mathcal{B} }\n",
    "\\newcommand{\\dd}{\\mathcal{D} }\n",
    "\\newcommand{\\g}{\\mathcal{G} }\n",
    "\\newcommand{\\p}{\\partial}\n",
    "\\newcommand{\\half}{\\frac{1}{2} }\n",
    "\\newcommand{\\T}{\\mathcal{T} }\n",
    "\\newcommand{\\bi}{\\begin{itemize}}\n",
    "\\newcommand{\\ei}{\\end{itemize}}\n",
    "\\newcommand{\\beq}{\\begin{equation}}\n",
    "\\newcommand{\\eeq}{\\end{equation}}\n",
    "\\newcommand{\\beas}{\\begin{eqnarray*}}\n",
    "\\newcommand{\\eeas}{\\end{eqnarray*}}\n",
    "\\newcommand{\\cO}{\\mathcal{O}}\n",
    "\\newcommand{\\cF}{\\mathcal{F}}\n",
    "\\newcommand{\\cL}{\\mathcal{L}}\n",
    "\\newcommand{\\BS}{\\text{BS}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of content\n",
    "- Introduction\n",
    "    * Delta Hedging\n",
    "    * Reinforcement Learning\n",
    "- Toolkits\n",
    "    * Gym\n",
    "    * Spinning Up\n",
    "    * Installation\n",
    "- Our Setting\n",
    "    * Learning Algorithms \n",
    "    * Models\n",
    "    * Formulation of The Problem\n",
    "    * The Implementation of Environments\n",
    "- Experiment\n",
    "    * Detailed Setting\n",
    "        - Parameters of Models\n",
    "        - Parameters of Environments\n",
    "    * The Results of Our Method\n",
    "    * The Results of Delta Hedging\n",
    "    * Comparison\n",
    "- Conclusion and Discussions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toolkits\n",
    "#### Gym\n",
    "The following content is from gym website: http://gym.openai.com\n",
    "- ' Gym is a toolkit for developing and comparing reinforcement learning algorithms. It makes no assumptions about the structure of your agent, and is compatible with any numerical computation library, such as TensorFlow or Theano.'\n",
    "-----------\n",
    "we use gym to implement our own environment, the environment code is also below, yet we need to do the following installation to follow the gym style but not simply define the function in the jupyter notebook\n",
    "\n",
    "#### Spinning up\n",
    "The following content is from Spinning up website: https://spinningup.openai.com\n",
    "- '...So our package here is designed to serve as the missing middle step for people who are excited by deep RL, and would like to learn how to use it or make a contribution, but don’t have a clear sense of what to study or how to transmute algorithms into code. We’ve tried to make this as helpful a launching point as possible.'\n",
    "----------------------\n",
    "we use spinningup to implement our algorithms, but we change their code, so we enclose this packages in the work dir.\n",
    "\n",
    "#### Installation\n",
    "The installation of spinup may take some time because it have many related packages, if you don't want to train the model yourself but just run our testing code, you at least need to have pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision#if you don't want to install spinup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -e spinup#for spinup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -e gym#for gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -e DHENV#install our own event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Setting\n",
    "#### Models\n",
    "Because ddpg is chosen as our algorithm, we need to model two functions. \n",
    "- The first function is the Q function, which is to map the action and state into the revenue\n",
    "- and the second is the P function, which is to map the state to the action. \n",
    "* Specifically, we all choose multiple perceptrons, the simplest deep learning network, as our modeling function. On the number of layers, we choose 4 (stock number, account deposit, stock price, maturity time) / 5 (plus actions) - > 80-- > 40 -- > 1, and add leakyrelu activation layer in the middle.Because ddpg is chosen as our algorithm, we need to model two functions. The first function is the Q function, which is to map the action and state into the revenue, and the second is the P function, which is to map the state to the action. Specifically, we all choose multiple perceptrons, the simplest deep learning network, as our modeling function. On the number of layers, we choose 4 (stock number, account deposit, stock price, maturity time) / 5 (plus actions) - > 80-- > 40 -- > 1, and add leakyrelu activation layer in the middle.**In particular, we add a tanh function at the end of P function. We will talk about its importance below. **The code is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "class HEDGING_Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, act_limit):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, 80)\n",
    "        self.ac1 = nn.LeakyReLU(0.1)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.ac2 = nn.LeakyReLU(0.1)\n",
    "        self.fc3 = nn.Linear(80, 40)\n",
    "        self.ac3 = nn.LeakyReLU(0.1)\n",
    "        self.fc4 = nn.Linear(40, act_dim)\n",
    "        self.th = nn.Tanh()\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Return output from network scaled to action space limits.\n",
    "        c = obs.numpy()\n",
    "        if np.isnan(c).any():\n",
    "            print('nan')\n",
    "            exit()\n",
    "        for item in c:\n",
    "            try:\n",
    "                for it in item:\n",
    "                    if np.inf == abs(it):\n",
    "                        print('inf')\n",
    "                        print(c)\n",
    "                        exit()\n",
    "            except:\n",
    "                 if np.inf == abs(item):\n",
    "                     print('inf')\n",
    "                     print(c)\n",
    "                     exit()\n",
    "        x = self.fc1(obs)\n",
    "        x = self.ac1(x)\n",
    "        '''\n",
    "        x = self.fc2(x)\n",
    "        x = self.ac2(x)\n",
    "        '''\n",
    "        x = self.fc3(x)\n",
    "        x = self.ac3(x)\n",
    "        x = self.fc4(x)\n",
    "        d = x.detach().numpy()\n",
    "        if np.isnan(d).any():\n",
    "            print('nan')\n",
    "            exit()\n",
    "        #x = torch.clamp(x, min = -5, max = 10)\n",
    "        return self.th(x)\n",
    "\n",
    "class HEDGING_QFunction(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim + act_dim, 80)\n",
    "        self.ac1 = nn.LeakyReLU(0.1)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.ac2 = nn.LeakyReLU(0.1)\n",
    "        self.fc3 = nn.Linear(80, 40)\n",
    "        self.ac3 = nn.LeakyReLU(0.1)\n",
    "        self.fc4 = nn.Linear(40, act_dim)\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        x = self.fc1(torch.cat([obs, act], dim=-1))\n",
    "        x = self.ac1(x)\n",
    "        '''\n",
    "        x = self.fc2(x)\n",
    "        x = self.ac2(x)\n",
    "        '''\n",
    "        x = self.fc3(x)\n",
    "        x = self.ac3(x)\n",
    "        x = self.fc4(x)\n",
    "        return torch.squeeze(x, -1) # Critical to ensure q has right shape.\n",
    "\n",
    "class HEDGING(nn.Module):\n",
    "\n",
    "    def __init__(self, observation_space, action_space,activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "\n",
    "        obs_dim = observation_space.shape[0]\n",
    "        act_dim = action_space.shape[0]\n",
    "        act_limit = action_space.high[0]\n",
    "\n",
    "        # build policy and value functions\n",
    "        self.pi = HEDGING_Actor(obs_dim, act_dim, act_limit)\n",
    "        self.q = HEDGING_QFunction(obs_dim, act_dim)\n",
    "\n",
    "    def act(self, obs):\n",
    "        with torch.no_grad():\n",
    "            return self.pi(obs).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reasons why the model is like that is below:\n",
    "- The number of layers of the model and the number of neurons: These are the results of experiment evaluated by convergence and rewards. In the code, you can see some traces of experiments that have not been deleted. Of course, due to the limitation of time, we did not make a more detailed parameter adjustment. \n",
    "- The tanh function: This function aim to solve the problem of the infinity action and reward. The function of tanh is - 1 to 1, which makes the range of action very stable. If the tanh function is not added, it is easy to have infinity, which will cause nan to appear in the training parameters.\n",
    "- Why tanh but not other: Your next question may be why you don't use other methods to solve the above problem. For example, if the action is forced to be limited to a certain range, this method will prevent the function from gradient descent. Intuitively, you can understand this thing: suppose the output value is 10000, However, we limit the action to - 5 to 5. At this time, the gradient around 10000 is 0, because whether we change 10000 into 9000 or 11000, the final benefit is the benefit with action of 5, so this method is not acceptable. Compared with the activation function of sigmoid, the range of sigmoid is 0 to 1, and tanh is from - 1 to + 1, which is more suitable for the problem of positive and negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "#### Training\n",
    "Training take about 7 hours, so if you do not have time do not run the cell below or you can just run to test if it run well. , and can load the network straightly from the dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spinup import ddpg_pytorch as ddpg#run it freely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actat, ac = ddpg(env_fn=lambda : gym.make('DHENV:DHENV-v1'),pi_lr = 1e-4, q_lr = 1e-4, actor_critic = HEDGING, num_test_episodes=100)#run it to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actat = torch.load('actar.pkl')\n",
    "ac = torch.load('ac.pkl')#run it to load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may ask why there are two networks here. This is because in the ddpg algorithm, there is a training network and a final network. Because the final network is only partially updated with the parameters of the training network, it receives the initial difference model with little influence, but there is less training. Let's take a look at their performance separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "Next, we use Monte Carlo simulation to conduct 10000 rounds of simulation tests on the two networks and delta hedging strategy respectively. Through these data, we evaluate their behavior from the two dimensions of discounted profit & loss and the final accounts value. Finally, we show their different behavior logic and try to explain the different values，**Monte Carlo simulation takes about an hour. If you don't have enough time, you can also use the data we have stored directly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run simulation\n",
    "Accounts = {}\n",
    "Rewards = {}\n",
    "actions = {}\n",
    "Prices = {}\n",
    "testlist = ['ac','actar','delta']\n",
    "for item in testlist:\n",
    "    Accounts[item] = []\n",
    "    actions[item] = []\n",
    "    Rewards[item] = []\n",
    "    Prices[item] = []\n",
    "for item in testlist:\n",
    "    if item == 'ac':\n",
    "        actitem = ac\n",
    "        env = gym.make('DHENV:DHENV-v1')\n",
    "    elif item == 'actar':\n",
    "        actitem = actat\n",
    "        env = gym.make('DHENV:DHENV-v1')\n",
    "    else:\n",
    "        actitem = ac\n",
    "        env = gym.make('DHENV:DHENV-v3')\n",
    "    for j in range(10000):\n",
    "        print(j)\n",
    "        state = env.reset()\n",
    "        act = actitem.act(torch.from_numpy(state))\n",
    "        for i in range(200):\n",
    "            s, r, d, _ = env.step(act)\n",
    "            if d:\n",
    "                break\n",
    "            act = actitem.act(torch.from_numpy(s))\n",
    "        Accounts[item].append(copy.deepcopy(env.Accounts))\n",
    "        actions[item].append(copy.deepcopy(env.actions))\n",
    "        Rewards[item].append(copy.deepcopy(env.rewards))\n",
    "        Prices[item].append(copy.deepcopy(env.prices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Accounts.pkl'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-67cf429f8af6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accounts.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mAccounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Prices.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mPrices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Accounts.pkl'"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "with open('Accounts.pkl','rb') as f:\n",
    "    Accounts = pickle.load(f)\n",
    "with open('Prices.pkl','rb') as f:\n",
    "    Prices = pickle.load(f)\n",
    "with open('Rewards.pkl','rb') as f:\n",
    "    Rewards = pickle.load(f)\n",
    "with open('actions.pkl','rb') as f:\n",
    "    actions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "actat = torch.load('actar.pkl')\n",
    "actat = torch.load('actar.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and Discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('python': conda)",
   "language": "python",
   "name": "python37764bitpythonconda11ea8fec7a3f439b94ba0c1adc3469e1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}